# -*- coding: utf-8 -*-
"""Emotion Detection VGG Net Good Dataset.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1YQdjN40rxFI7-jaGeFdJPrXFaCdq11b-
"""

# Commented out IPython magic to ensure Python compatibility.
# Import Statements
import torch
import torch.nn as nn
from torch.utils.data import Dataset, DataLoader
import numpy as np
from keras.preprocessing import image
import tensorflow as tf
import pandas as pd
from tqdm import tqdm
from PIL import Image as im
import random
import matplotlib 
import matplotlib.pyplot as plt
import torchvision
import math
import pickle as pkl
import numpy as np
from sklearn.model_selection import KFold
from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay
from sklearn.model_selection import train_test_split
import seaborn as sn
# %matplotlib inline

if torch.cuda.is_available():
  device = torch.device("cuda")
else:
  device = torch.device("cpu")

!pip install kaggle

from google.colab import drive
drive.mount('/content/drive')

path  = '/content/drive/MyDrive/389Class/Final_Project'

# Commented out IPython magic to ensure Python compatibility.
# %cd /content/drive/MyDrive/389Class/Final_Project

# ! pip install kaggle
#

# ! chmod 600 ~/.kaggle/kaggle.json

# ! cp kaggle.json ~/.kaggle/

# !nvidia-smi

# # https://www.kaggle.com/questions-and-answers/135334
# # Loading the dataset
# total_dataset = pd.read_csv('fer2013.csv')
# # reading the training images
# total_image = []
# # total_dataset.shape is (35887, 3)
# print(total_dataset.keys())
# imgList = []
# # np.zeros((48,48),dtype = int) for _ in range(len(total_dataset['pixels']))
# labels = []
# for label in total_dataset['emotion']:
#     labels.append(label)
# index = 0
# for row in total_dataset['pixels']:
#     rows = row.split()
#     img = np.asarray([int(val) for val in rows])
#     # print(img.shape)
#     if(img.shape[0] != 2304):
#       continue
#     img = np.reshape(img, (48,48))
#     img = torch.from_numpy(img)
#     imgList.append([img.view(-1,1,48,48),labels[index]])
#     index += 1
    
# imgList of format [image, label]

# save = True
# loadimgList = True
# # Saving the Variable 
# path = ""
# fileObject = open('imgList.pkl', 'wb')

# if save:
#     pkl.dump(imgList, fileObject)
#     fileObject.close()

# if loadimgList:
#     fileObject2 = open('imgList.pkl', 'rb')
#     modelInput = pkl.load(fileObject2)
#     fileObject2.close()

#Read dataset
df = pd.read_csv('fer2013.csv')

#splitting the dataset into train and test
X = []
for row in df['pixels']:
  rows = row.split()
  img = np.asarray([int(val) for val in rows])
  X.append(img)

Y= df["emotion"]

X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, shuffle=True)

# print(X_train)
print(Y_train)

# Creating the Dataset:
class MyDataset(Dataset):
    def __init__(self):
      # https://www.kaggle.com/questions-and-answers/135334
        # Loading the dataset
        self.df = pd.read_csv('fer2013.csv')
        # reading the training images
        self.x = []
        self.y = []
        # total_image = []
        # total_dataset.shape is (35887, 3)
        # print(total_dataset.keys())
        # imgList = []
        for label in self.df['emotion']:
            self.y.append(label)
        # index = 0
        for row in self.df['pixels']:
            rows = row.split()
            img = np.asarray([int(val) for val in rows])
            # print(img.shape)
            if img.shape[0] != 2304:
              continue
            img = np.reshape(img, (48,48))
            img = torch.from_numpy(img)
            self.x.append(img)
            # imgList.append([img.view(-1,1,48,48),labels[index]])
        #     # index += 1
    
        # self.df = pd.read_csv(root)
        # self.data = self.df.to_numpy()
        # self.x , self.y = (torch.from_numpy(self.data[:,:n_inp]),
        #                    torch.from_numpy(self.data[:,n_inp:]))
    def __getitem__(self, idx):
      if idx >= len(self.x):
        print(idx)
      return self.x[idx], self.y[idx]
    def __len__(self):
        return len(self.df)

dataset = MyDataset()

# Dataset )
trainSize = round(0.85 * dataset.__len__())
testSize = dataset.__len__() - trainSize
print(dataset.__len__())
train_set, val_set = torch.utils.data.random_split(dataset, [trainSize, testSize])
#

print(dataset.__len__())

print(len(dataset))
print(trainSize)
print(testSize)
print(len(val_set))
print(len(train_set))
# print(dataset.x[0])

print(dataset.__len__())
print(dataset.__getitem__(30001))

# Train Dataloader
batch_size = 1000
# data_loader = DataLoader(train_set, batch_size = batch_size, shuffle=True)
# Test Dataloader
test_dataloader = DataLoader(train_set, batch_size=batch_size, shuffle=True)

# # Splitting into Test and Train
# # 35887 : 30504 ie 85% of total  + 5383 (15 % of total as test)
# # Remaining must be test dataset.
# # train_ds, test_ds = torch.utils.data.random_split(data_loader.dataset, (30504,5383)) 
# from torch.utils.data import DataLoader, Subset
# from sklearn.model_selection import train_test_split

# TEST_SIZE = 30504
# BATCH_SIZE = 64
# SEED = 42

# # generate indices: instead of the actual data we pass in integers instead
# train_indices, test_indices, _, _ = train_test_split(
#     range(len(dataset)),
#     dataset.x,
#     stratify=dataset.x,
#     test_size=TEST_SIZE
# )

# # generate subset based on indices
# train_split = Subset(dataset, train_indices)
# test_split = Subset(dataset, test_indices)

# # create batches
# train_batches = DataLoader(train_split, batch_size=BATCH_SIZE, shuffle=True)
# test_batches = DataLoader(test_split, batch_size=BATCH_SIZE, shuffle=True)

# print(train_set.indices)
# # train_dataset = imgList[:17412]
# # # train : 70 % of total dataset
# # test_dataset = imgList[17412:19100]
# # # test : 15 % of total dataset
# # validation_dataset = imgList[19100:]
# # # ending size for validation set 35887
# # # Shuffle Everytime

emotion = {0 : 'sad', 1 : 'fear', 2: 'disgust', 3: 'anger',4 : 'neutral',5: 'surprise',6:'happy'}
# neutral, happy, surprise, sad, fear, disgust, and anger

# VGG Net Architecture
# 4 convolutional stage followed by 3 fully connected layers
# Each convolutional stage consists of 2 convolutional blocks and a max pooling layer
# Each convolutional block consists of a convolitional layer, ReLU and a Batch normalization layer
# A relu after the first two fully connected layers and then just the final fully connected layer for classification

# 4 convolutional stage , 3 fully connected layers

class vggNet(nn.Module):
    """
    This is our Basic CNN model which we plan to build upon
    """
    def __init__(self,input_size,output_shape):
      # Input image is of 1 x 48 x 48
      # output_size = ((input_size + 2 * padding - kernel_size) / stride) + 1
        super().__init__()
        self.input_size = input_size
        # stage 1
        # block 1
        self.conv1 = nn.Conv2d(in_channels = 1, out_channels = 3, kernel_size = 4, stride = 1, padding = 0);
        self.batchNorm1 = nn.BatchNorm2d(3);
        #block 2
        self.conv2 = nn.Conv2d(in_channels = 3, out_channels = 3, kernel_size = 4, stride = 1, padding = 0);
        self.batchNorm2 = nn.BatchNorm2d(3);
        self.maxPool1 = nn.MaxPool2d((4,4), stride=1)

        # stage 2
        # block 1
        self.conv3 = nn.Conv2d(in_channels = 3, out_channels = 3, kernel_size = 4, stride = 1, padding = 0);
        self.batchNorm3 = nn.BatchNorm2d(3);
        #block 2
        self.conv4 = nn.Conv2d(in_channels = 3, out_channels = 3, kernel_size = 4, stride = 1, padding = 0);
        self.batchNorm4 = nn.BatchNorm2d(3);
        self.maxPool2 = nn.MaxPool2d((4,4), stride=1)

        # stage 1
        # block 1
        self.conv5 = nn.Conv2d(in_channels = 3, out_channels = 3, kernel_size = 4, stride = 1, padding = 0);
        self.batchNorm5 = nn.BatchNorm2d(3);
        #block 2
        self.conv6 = nn.Conv2d(in_channels = 3, out_channels = 3, kernel_size = 4, stride = 1, padding = 0);
        self.batchNorm6 = nn.BatchNorm2d(3);
        self.maxPool3 = nn.MaxPool2d((4,4), stride=1)

        # stage 1
        # block 1
        self.conv7 = nn.Conv2d(in_channels = 3, out_channels = 3, kernel_size = 4, stride = 1, padding = 0);
        self.batchNorm7 = nn.BatchNorm2d(3);
        #block 2
        self.conv8 = nn.Conv2d(in_channels = 3, out_channels = 3, kernel_size = 4, stride = 1, padding = 0);
        self.batchNorm8 = nn.BatchNorm2d(3);
        self.maxPool4 = nn.MaxPool2d((4,4), stride=1)

#       Output size expected after conv layers
        self.cnn_out_size = 3 * 12 * 12
        self.lin1 = nn.Linear(self.cnn_out_size,150)
        self.lin2 = nn.Linear(150, 75)
        self.lin3 = nn.Linear(75, output_shape)
        self.relu = nn.ReLU()
        self.sigmoid = nn.Sigmoid()

    def forward(self,x):
#         print(type(x))
        x = x.view(-1, 1, 48, 48).float()
        out = x
        # stage 1
        # print("Stage 1")
        # block 1
        out = self.conv1(out)
        out = self.relu(out)
        out = self.batchNorm1(out)
        # print("Block 1 Convolution Layer 1: ", out.shape)

        # block 2
        out = self.conv2(out)
        out = self.relu(out)
        out = self.batchNorm2(out)
        # print("Block 2 Convolution Layer 1: ", out.shape)

        # MaxPool Layer
        out = self.maxPool1(out)
        # print("Max Pool Layer 1: ", out.shape)

        # stage 2
        # print("Stage 2")
        # block 1
        out = self.conv3(out)
        out = self.relu(out)
        out = self.batchNorm3(out)
        # print("Block 1 Convolution Layer 1: ", out.shape)

        # block 2
        out = self.conv4(out)
        out = self.relu(out)
        out = self.batchNorm4(out)
        # print("Block 2 Convolution Layer 1: ", out.shape)

        # MaxPool Layer
        out = self.maxPool2(out)
        # print("Max Pool Layer 2: ", out.shape)

        # stage 3
        # print("Stage 3")
        # block 1
        out = self.conv5(out)
        out = self.relu(out)
        out = self.batchNorm5(out)
        # print("Block 1 Convolution Layer 1: ", out.shape)

        # block 2
        out = self.conv6(out)
        out = self.relu(out)
        out = self.batchNorm6(out)
        # print("Block 2 Convolution Layer 1: ", out.shape)

        # MaxPool Layer
        out = self.maxPool3(out)
        # print("Max Pool Layer 3: ", out.shape)

        # stage 4
        # print("Stage 4")
        # block 1
        out = self.conv7(out)
        out = self.relu(out)
        out = self.batchNorm7(out)
        # print("Block 2 Convolution Layer 1: ", out.shape)

        # block 2
        out = self.conv8(out)
        out = self.relu(out)
        out = self.batchNorm8(out)
        # print("Block 2 Convolution Layer 1: ", out.shape)

        # MaxPool Layer
        out = self.maxPool4(out)
        # print("Max Pool Layer 4: ", out.shape)

        # layer 1
        out = out.flatten(start_dim = 1)
        out = self.lin1(out)
        out = self.relu(out)
        # print("Linear Layer 1 Result: ", out.shape)

        # layer 2
        out = self.lin2(out)
        out = self.relu(out)
        # print("Linear Layer 2 Result: ", out.shape)

        # layer 3
        out = self.lin3(out)
        out = self.sigmoid(out)
        # print("Linear Layer 3 Result: ", out.shape)
        return out

model = vggNet((1, 48, 48), 7)
trainImage = dataset.__getitem__(9)[0]
test_out = model(trainImage)
print("Model output shape: ",test_out.shape)

print(test_out)
print(torch.argmax(dim = 1, input = test_out))

def NN_training(model, loss_function, optimizer, train_data, n_epochs, update_interval):

    '''
    Updates the parameters of the given model using the optimizer of choice to
    reduce the given loss_function

    This will iterate over the dataloader 'n_epochs' times training on each batch of images
    
    To get the gradient (which is stored internally in the model) use .backward() from the loss tensor
    and to apply it use .step() on the optimizer

    In between steps you need to zero the gradient so it can be recalculated -- use .zero_grad for this
    '''
    
    losses = []
    lossPart = []
    losses2 = []

    y_test = []
    y_pred = []


    for n in range(n_epochs):
        
        for i, (image,label) in enumerate(tqdm(train_data)):
            ##############################################################
            optimizer.zero_grad()
            my_output = model(image)
            tens_label = torch.tensor(label).long()
            loss = loss_function(my_output,tens_label)
# torch.Tensor([label]).long()
            y_test.append(label)
            test_out = torch.argmax(dim = 1, input = my_output)
            y_pred.append(int(test_out[0].numpy()))

            losses2.append(loss.item())
            lossPart.append(loss.item())
            loss.backward()
            optimizer.step()
            ##############################################################
            if i % update_interval == 0:
                losses.append(sum(lossPart)/len(lossPart)) # This will append your losses for plotting -- please use "loss" as the name for your loss
                lossPart = []
    return model, losses,losses2, y_test, y_pred

# Plug in your model, loss function, and optimizer 
# Try out different hyperparameters and different models to see how they perform

lr = 0.0009             # The size of the step taken when doing gradient descent
batch_size = 1000  # The number of images being trained on at once
update_interval = 1000    # The number of batches trained on before recording loss
n_epochs = 60      # The number of times we train through the entire dataset

output_size = 7    # This is the size of the output of the model 
input_shape = (1,48,48)

dataset = train_set    # The dataset is a pain to load/unload so we want to keep it active
train_dataloader = DataLoader(train_set, batch_size = batch_size, shuffle=True)
loss_function = nn.CrossEntropyLoss()     
# model = vggNet(input_shape, output_size) 
# loss_function = nn.CrossEntropyLoss()                     
# optimizer = torch.optim.Adam(model.parameters(), lr=lr) 
# trained_model, losses, losses2, y_test, y_pred = NN_training(model, loss_function, optimizer, train_dataloader, n_epochs=n_epochs, update_interval=update_interval)
# modelSaveName = "TenEModel1.pt"
# torch.save(trained_model.state_dict(), modelSaveName)
# print(losses)
# plt.plot(len(losses2),losses2)
# plt.title("training curve Losses2")
# plt.xlabel("number of images trained on")
# plt.ylabel("Reconstruction loss")
# plt.show()


# plt.plot(losses)
# plt.title("training curve")
# plt.xlabel("number of epochs trained on")
# plt.ylabel("Reconstruction loss")
# plt.show()


# NOTE: It will take a while for this to train (depending on your model)
# You can increase the batch size (way up top) or reduce the size of your model if it takes too long

modelLoadName = "FortyEModel1.pt"
modelSaveName = "HundredEModel1.pt"

model = vggNet(input_shape, output_size)                 
# model.load_state_dict(torch.load(modelLoadName))
optimizer = torch.optim.Adam(model.parameters(), lr=lr) 
trained_model, losses, losses2, y_test, y_pred = NN_training(model, loss_function, optimizer, train_dataloader, n_epochs=n_epochs, update_interval=update_interval)
torch.save(trained_model.state_dict(), modelSaveName)

trained_model, losses, losses2, y_test, y_pred = NN_training(model, loss_function, optimizer, train_dataloader, n_epochs=n_epochs, update_interval=update_interval)
torch.save(trained_model.state_dict(), modelSaveName)

plt.plot(losses)
plt.title("training curve")
plt.xlabel("number of Instances trained on")
plt.ylabel("Reconstruction loss")
plt.show()

t_model = vggNet((1,48,48),7)
t_model.load_state_dict(torch.load(modelSaveName))
# t_model = torch.load(modelLoadName)                    

# model.eval()

print(torch.tensor(losses).mean())

# losses_new = np.mean(np.array(losses2[0:25000]).reshape(-1, 50), axis=1)
# plt.plot(range(0, len(losses2[0:25000]), 50), losses_new)

def testing(model, loss_function, test_data):

    '''
    This function will test the given model on the given test_data
    it will return the accuracy and the test loss (given by loss_function) 
    '''
    
    sum_loss = 0
    n_correct = 0
    total = 0
    y_test = []
    y_pred = []
    for i, (image, label) in enumerate(tqdm(iter(test_data))):

        # This is essentially exactly the same as the training loop 
        # without the, well, training, part
        pred = model(image)
        for p in pred:
          y_pred.append(p)
        tens_label = torch.tensor(label).long()
        for l in tens_label:
          y_test.append(l)
        loss = loss_function(pred, tens_label)
        sum_loss += loss.item()

        _, predicted = torch.max(pred,1)
        n_correct += (predicted == tens_label).sum()
        total += tens_label.size(0)
    
    test_acc = round(((n_correct / total).item() * 100), 2)
    avg_loss = round(sum_loss / len(test_data), 2)

    print("accuracy:", test_acc)
    print("loss:", avg_loss )

    return test_acc, avg_loss, y_pred, y_test

print("Train Accuracy and Loss")
train_acc, train_loss, tr_test, tr_pred = testing(t_model, loss_function, train_dataloader)

print("Test Accuracy and Loss")
test_acc, test_loss, t_test, t_pred = testing(t_model, loss_function, test_dataloader)

labels=[0,1,2,3,4,5,6]
cm = confusion_matrix(y_test, y_pred, labels=labels)
disp = ConfusionMatrixDisplay(confusion_matrix=cm,display_labels=labels)
disp.plot()
plt.show()

# Hyper parameter Search :
def hyperparamSearch():
  learning_rate = [0.0008, 0.0009]
  # n_epochs = [30,40,50,60]
  n_epochs=[20,30,40]
  batch_size = 1000
  train_dataloader = DataLoader(train_set, batch_size = batch_size, shuffle=True)
  model = vggNet((1,48,48), 7) 
  update_interval = 1000
  loss_function = nn.CrossEntropyLoss()
  best_lr = 0
  best_epoch = 0
  best_acc = 0
  for lr in learning_rate:
      optimizer = torch.optim.Adam(model.parameters(), lr=lr) 
      model = vggNet((1,48,48), 7)
      for n in n_epochs: 
        trained_model, losses, losses2, y_test, y_pred = NN_training(model, loss_function, optimizer, train_dataloader, n_epochs=n, update_interval=update_interval)
        print(lr, n,"lr,n")
        test_acc, test_loss, t_test, t_pred = testing(trained_model, loss_function, test_dataloader)
        if test_acc >= best_acc:
          best_lr = lr
          best_epoch = n
  return best_lr, best_epoch
  # best (0.0001, 20)
  # learning_rate = [1e-3,1e-2,1e-4]
  # n_epochs = [10, 5, 20]
  # batch_size = 1000

  # (0.0009, 30) 60%
  # learning_rate = [0.0003, 0.0007, 0.0009]
  # n_epochs = [10,20,30]
  # batch_size = 1000
  
  # (0.0009, 40) 45%
  # learning_rate = [0.001, 0.0007, 0.0009]
  # n_epochs = [20,30,40]
  # batch_size = 1000

hyperparamSearch()

# Open the image file
image = Image.open('Image9.png')
# Getting the mean and standard dev
# import necessary libraries
import torchvision.transforms as transforms
import matplotlib.pyplot as plt
  
# define custom transform function
transform = transforms.Compose([
    transforms.ToTensor()
])
  
# transform the pIL image to tensor 
# image
img_tr = transform(image)
  
# Convert tensor image to numpy array
img_np = np.array(img_tr)
  
# plot the pixel values
plt.hist(img_np.ravel(), bins=50, density=True)
plt.xlabel("pixel values")
plt.ylabel("relative frequency")
plt.title("distribution of pixels")

# Python code to calculate mean and std
# of image
  
# get tensor image
img_tr = transform(image)
  
# calculate mean and std
mean, std = img_tr.mean([1,2]), img_tr.std([1,2])
  
# print mean and std
print("mean and std before normalize:")
print("Mean of the image:", mean)
print("Std of the image:", std)

# Saliency Map

from torchvision import transforms as T
from PIL import Image



# # Set up the transformations
# transform_ = transforms.Compose([
# 		transforms.Resize(48),
# 		transforms.CenterCrop(48),
# 		transforms.Normalize(),
# 		transforms.ToTensor(),
# ])

# # Transforms the image
# image = transform_(image)
# # Reshape the image (because the model use 
# # 4-dimensional tensor (batch_size, channel, width, height))
# image = image.reshape(1, 1, 48, 48)
# # Set the device for the image
# image = image.to(device)
# # Set the requires_grad_ to the image for retrieving gradients
# image.requires_grad_()
# # Retrieve output from the image
# output = model(image)

# # Catch the output
# output_idx = output.argmax()
# output_max = output[0, output_idx]

# # Do backpropagation to get the derivative of the output based on the image
# output_max.backward()
# # Retireve the saliency map and also pick the maximum value from channels on each pixel.
# # In this case, we look at dim=1. Recall the shape (batch_size, channel, width, height)
# saliency, _ = torch.max(X.grad.data.abs(), dim=1) 
# saliency = saliency.reshape(224, 224)

# # Reshape the image
# image = image.reshape(-1, 224, 224)

# # Visualize the image and the saliency map
# fig, ax = plt.subplots(1, 2)
# ax[0].imshow(image.cpu().detach().numpy().transpose(1, 2, 0))
# ax[0].axis('off')
# ax[1].imshow(saliency.cpu(), cmap='hot')
# ax[1].axis('off')
# plt.tight_layout()
# fig.suptitle('The Image and Its Saliency Map')
# plt.show()


# # Retireve the saliency map and also pick the maximum value from channels on each pixel.
# # In this case, we look at dim=1. Recall the shape (batch_size, channel, width, height)
# saliency, _ = torch.max(X.grad.data.abs(), dim=1) 
# saliency = saliency.reshape(224, 224)

# # Reshape the image
# image = image.reshape(-1, 224, 224)

# # Visualize the image and the saliency map
# fig, ax = plt.subplots(1, 2)
# ax[0].imshow(image.cpu().detach().numpy().transpose(1, 2, 0))
# ax[0].axis('off')
# ax[1].imshow(saliency.cpu(), cmap='hot')
# ax[1].axis('off')
# plt.tight_layout()
# fig.suptitle('The Image and Its Saliency Map')
# plt.show()
# Preprocess the image
def preprocess(image, size=48):
    transform = T.Compose([
        T.Resize((size,size)),
        T.ToTensor(),
        T.Normalize(mean=[0.7235, 0.8602, 0.7826, 0.3840], std=[0.3896, 0.2110, 0.2904, 0.4854]),
        T.Lambda(lambda x: x[None]),
    ])
    return transform(image)

'''
    Y = (X - μ)/(σ) => Y ~ Distribution(0,1) if X ~ Distribution(μ,σ)
    => Y/(1/σ) follows Distribution(0,σ)
    => (Y/(1/σ) - (-μ))/1 is actually X and hence follows Distribution(μ,σ)
'''
def deprocess(image):
    transform = T.Compose([
        T.Lambda(lambda x: x[0]),
        T.Normalize(mean=[0, 0, 0], std=[4.3668, 4.4643, 4.4444]),
        T.Normalize(mean=[-0.485, -0.456, -0.406], std=[1, 1, 1]),
        T.ToPILImage(),
    ])
    return transform(image)

def show_img(PIL_IMG):
    plt.imshow(np.asarray(PIL_IMG))

# preprocess the image
X = preprocess(image)

# we would run the model in evaluation mode
model.eval()

# we need to find the gradient with respect to the input image, so we need to call requires_grad_ on it
X.requires_grad_()

'''
forward pass through the model to get the scores, note that VGG-19 model doesn't perform softmax at the end
and we also don't need softmax, we need scores, so that's perfect for us.
'''

scores = model(X)
print(scores)
# Get the index corresponding to the maximum score and the maximum score itself.
score_max_index = scores.argmax().numpy()
print(score_max_index)
score_max = scores[0,score_max_index]

'''
backward function on score_max performs the backward pass in the computation graph and calculates the gradient of 
score_max with respect to nodes in the computation graph
'''
score_max.backward()

'''
Saliency would be the gradient with respect to the input image now. But note that the input image has 3 channels,
R, G and B. To derive a single class saliency value for each pixel (i, j),  we take the maximum magnitude
across all colour channels.
'''
saliency, _ = torch.max(X.grad.data.abs(),dim=1)

# Reshape the image
# image = image.reshape(-1, 48, 48)

# Visualize the image and the saliency map
fig, ax = plt.subplots(1, 2)
ax[0].imshow(image)
ax[0].axis('off')
ax[1].imshow(saliency[0], cmap='cool')
ax[1].axis('off')
plt.tight_layout()
fig.suptitle('The Image and Its Saliency Map')
plt.show()
# # code to plot the saliency map as a heatmap
# plt.imshow(saliency[0], cmap=plt.cm.hot)
# plt.axis('off')
# plt.show()

# https://www.geeksforgeeks.org/how-to-normalize-images-in-pytorch/ 
# https://github.com/TejaGollapudi/PyTorch-CNN-Visualizations-Saliency-Maps
# https://towardsdatascience.com/saliency-map-using-pytorch-68270fe45e80 
# https://medium.datadriveninvestor.com/visualizing-neural-networks-using-saliency-maps-in-pytorch-289d8e244ab4